@article{solomon2015convolutional,
  title={Convolutional wasserstein distances: Efficient optimal transportation on geometric domains},
  author={Solomon, Justin and De Goes, Fernando and Peyr{\'e}, Gabriel and Cuturi, Marco and Butscher, Adrian and Nguyen, Andy and Du, Tao and Guibas, Leonidas},
  journal={ACM Transactions on Graphics (TOG)},
  volume={34},
  number={4},
  pages={1--11},
  year={2015},
  publisher={ACM New York, NY, USA}
}

@ARTICLE{uspsdataset,
  author={J. J. {Hull}},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A database for handwritten text recognition research}, 
  year={1994},
  volume={16},
  number={5},
  pages={550-554},
  doi={10.1109/34.291440}
} 

@article{Nichol2018OnFM,
  title={On First-Order Meta-Learning Algorithms},
  author={Alex Nichol and Joshua Achiam and John Schulman},
  journal={ArXiv},
  year={2018},
  volume={abs/1803.02999}
}


@article{SCHIEBINGER2019928,
title = {Optimal-Transport Analysis of Single-Cell Gene Expression Identifies Developmental Trajectories in Reprogramming},
journal = {Cell},
volume = {176},
number = {4},
pages = {928-943.e22},
year = {2019},
issn = {0092-8674},
doi = {https://doi.org/10.1016/j.cell.2019.01.006},
url = {https://www.sciencedirect.com/science/article/pii/S009286741930039X},
author = {Geoffrey Schiebinger and Jian Shu and Marcin Tabaka and Brian Cleary and Vidya Subramanian and Aryeh Solomon and Joshua Gould and Siyan Liu and Stacie Lin and Peter Berube and Lia Lee and Jenny Chen and Justin Brumbaugh and Philippe Rigollet and Konrad Hochedlinger and Rudolf Jaenisch and Aviv Regev and Eric S. Lander}
}

@inproceedings{jiang2020wasserstein,
  title={Wasserstein fair classification},
  author={Jiang, Ray and Pacchiano, Aldo and Stepleton, Tom and Jiang, Heinrich and Chiappa, Silvia},
  booktitle={Uncertainty in Artificial Intelligence},
  pages={862--872},
  year={2020},
  organization={PMLR}
}


@article{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{garnelo2018conditional,
  title={Conditional neural processes},
  author={Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, SM Ali},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2018},
  organization={PMLR}
}

@article{garnelo2018neural,
  title={Neural processes},
  author={Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1807.01622},
  year={2018}
}

@InProceedings{pmlr-v78-finn17a,
  title = 	 {One-Shot Visual Imitation Learning via Meta-Learning},
  author = 	 {Finn, Chelsea and Yu, Tianhe and Zhang, Tianhao and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = 	 {357--368},
  year = 	 {2017},
  editor = 	 {Levine, Sergey and Vanhoucke, Vincent and Goldberg, Ken},
  volume = 	 {78},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--15 Nov},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v78/finn17a/finn17a.pdf},
  url = 	 {https://proceedings.mlr.press/v78/finn17a.html},
  abstract = 	 {In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.}
}


@InProceedings{pmlr-v70-finn17a,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author =       {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}


@misc{
lacombe2021learning,
title={Learning to generate Wasserstein barycenters},
author={Julien Lacombe and Julie Digne and Nicolas Courty and Nicolas Bonneel},
year={2021},
url={https://openreview.net/forum?id=2ioNazs6lvw}
}

@InProceedings{pmlr-v84-genevay18a,
  title = 	 {Learning Generative Models with Sinkhorn Divergences},
  author = 	 {Genevay, Aude and Peyre, Gabriel and Cuturi, Marco},
  booktitle = 	 {Proceedings of the Twenty-First International Conference on Artificial Intelligence and Statistics},
  pages = 	 {1608--1617},
  year = 	 {2018},
  editor = 	 {Storkey, Amos and Perez-Cruz, Fernando},
  volume = 	 {84},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--11 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v84/genevay18a/genevay18a.pdf},
  url = 	 {https://proceedings.mlr.press/v84/genevay18a.html},
  abstract = 	 {The ability to compare two degenerate probability distributions, that is two distributions supported on low-dimensional manifolds in much higher-dimensional spaces, is a crucial factor in the estimation of generative mod- els.It is therefore no surprise that optimal transport (OT) metrics and their ability to handle measures with non-overlapping sup- ports have emerged as a promising tool. Yet, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational bur- den of evaluating OT losses, (ii) their instability and lack of smoothness, (iii) the difficulty to estimate them, as well as their gradients, in high dimension. This paper presents the first tractable method to train large scale generative models using an OT-based loss called Sinkhorn loss which tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into a differentiable and more robust quantity that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations with seam- less GPU execution. Additionally, Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Energy distance/Maximum Mean Discrepancy (MMD) losses, thus allowing to find a sweet spot leveraging the geometry of OT on the one hand, and the favorable high-dimensional sample complexity of MMD, which comes with un- biased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.}
}


@phdthesis{amos2019differentiable,
  title={Differentiable optimization-based modeling for machine learning},
  author={Amos, Brandon},
  year={2019},
  school={PhD thesis. Carnegie Mellon University}
}

@article{kolouri2019generalized,
  title={Generalized sliced Wasserstein distances},
  author={Kolouri, Soheil and Nadjahi, Kimia and Simsekli, Umut and Badeau, Roland and Rohde, Gustavo K},
  journal={arXiv preprint arXiv:1902.00434},
  year={2019}
}

@inproceedings{deshpande2019max,
  title={Max-sliced wasserstein distance and its use for gans},
  author={Deshpande, Ishan and Hu, Yuan-Ting and Sun, Ruoyu and Pyrros, Ayis and Siddiqui, Nasir and Koyejo, Sanmi and Zhao, Zhizhen and Forsyth, David and Schwing, Alexander G},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10648--10656},
  year={2019}
}

@article{fulop2021efficient,
  title={Efficient estimates of optimal transport via low-dimensional embeddings},
  author={Fulop, Patric M and Danos, Vincent},
  journal={arXiv preprint arXiv:2111.04838},
  year={2021}
}

@article{li2016learning,
  title={Learning to optimize},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1606.01885},
  year={2016}
}

@inproceedings{amos2020differentiable,
  title={The differentiable cross-entropy method},
  author={Amos, Brandon and Yarats, Denis},
  booktitle={International Conference on Machine Learning},
  pages={291--302},
  year={2020},
  organization={PMLR}
}

@article{fickinger2021cross,
  title={Cross-Domain Imitation Learning via Optimal Transport},
  author={Fickinger, Arnaud and Cohen, Samuel and Russell, Stuart and Amos, Brandon},
  journal={arXiv preprint arXiv:2110.03684},
  year={2021}
}

@article{venkataraman2021neural,
  title={Neural Fixed-Point Acceleration for Convex Optimization},
  author={Venkataraman, Shobha and Amos, Brandon},
  journal={arXiv preprint arXiv:2107.10254},
  year={2021}
}

@article{seguy2017large,
  title={Large-scale optimal transport and mapping estimation},
  author={Seguy, Vivien and Damodaran, Bharath Bhushan and Flamary, R{\'e}mi and Courty, Nicolas and Rolet, Antoine and Blondel, Mathieu},
  journal={arXiv preprint arXiv:1711.02283},
  year={2017}
}

@article{rout2021generative,
  title={Generative Modeling with Optimal Transport Maps},
  author={Rout, Litu and Korotin, Alexander and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2110.02999},
  year={2021}
}

@article{litvinenko2021computing,
  title={Computing f-Divergences and Distances of High-Dimensional Probability Density Functions--Low-Rank Tensor Approximations},
  author={Litvinenko, Alexander and Marzouk, Youssef and Matthies, Hermann G and Scavino, Marco and Spantini, Alessio},
  journal={arXiv preprint arXiv:2111.07164},
  year={2021}
}

@article{scetbon2021low,
  title={Low-Rank Sinkhorn Factorization},
  author={Scetbon, Meyer and Cuturi, Marco and Peyr{\'e}, Gabriel},
  journal={arXiv preprint arXiv:2103.04737},
  year={2021}
}

@inproceedings{forrow2019statistical,
  title={Statistical optimal transport via factored couplings},
  author={Forrow, Aden and H{\"u}tter, Jan-Christian and Nitzan, Mor and Rigollet, Philippe and Schiebinger, Geoffrey and Weed, Jonathan},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2454--2465},
  year={2019},
  organization={PMLR}
}

@article{scetbon2021linear,
  title={Linear-Time Gromov Wasserstein Distances using Low Rank Couplings and Costs},
  author={Scetbon, Meyer and Peyr{\'e}, Gabriel and Cuturi, Marco},
  journal={arXiv preprint arXiv:2106.01128},
  year={2021}
}

@article{muzellec2019subspace,
  title={Subspace detours: Building transport plans that are optimal on subspace projections},
  author={Muzellec, Boris and Cuturi, Marco},
  journal={arXiv preprint arXiv:1905.10099},
  year={2019}
}

@article{xu2019scalable,
  title={Scalable Gromov-Wasserstein learning for graph partitioning and matching},
  author={Xu, Hongteng and Luo, Dixin and Carin, Lawrence},
  journal={Advances in neural information processing systems},
  volume={32},
  pages={3052--3062},
  year={2019}
}

@incollection{merigot2021optimal,
  title={Optimal transport: discretization and algorithms},
  author={Merigot, Quentin and Thibert, Boris},
  booktitle={Handbook of Numerical Analysis},
  volume={22},
  pages={133--212},
  year={2021},
  publisher={Elsevier}
}

@article{korotin2021neural,
  title={Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark},
  author={Korotin, Alexander and Li, Lingxiao and Genevay, Aude and Solomon, Justin and Filippov, Alexander and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2106.01954},
  year={2021}
}

@article{korotin2019wasserstein,
  title={Wasserstein-2 generative networks},
  author={Korotin, Alexander and Egiazarian, Vage and Asadulaev, Arip and Safin, Alexander and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:1909.13082},
  year={2019}
}

@article{mokrov2021large,
  title={Large-Scale Wasserstein Gradient Flows},
  author={Mokrov, Petr and Korotin, Alexander and Li, Lingxiao and Genevay, Aude and Solomon, Justin and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2106.00736},
  year={2021}
}

@article{li2020continuous,
  title={Continuous regularized Wasserstein barycenters},
  author={Li, Lingxiao and Genevay, Aude and Yurochkin, Mikhail and Solomon, Justin},
  journal={arXiv preprint arXiv:2008.12534},
  year={2020}
}

@article{korotin2021continuous,
  title={Continuous wasserstein-2 barycenter estimation without minimax optimization},
  author={Korotin, Alexander and Li, Lingxiao and Solomon, Justin and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2102.01752},
  year={2021}
}

@book{villani2009optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2009},
  publisher={Springer}
}

@article{cuturi2013sinkhorn,
  title={Sinkhorn distances: Lightspeed computation of optimal transport},
  author={Cuturi, Marco},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={2292--2300},
  year={2013}
}

@article{amos2022tutorial,
  title={Tutorial on amortized optimization for learning to optimize over continuous domains},
  author={Amos, Brandon},
  journal={arXiv preprint arXiv:2202.00665},
  year={2022}
}

@article{korotin2022neural,
  title={Neural Optimal Transport},
  author={Korotin, Alexander and Selikhanovych, Daniil and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2201.12220},
  year={2022}
}

@article{peyre2019computational,
  title={Computational optimal transport: With applications to data science},
  author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={11},
  number={5-6},
  pages={355--607},
  year={2019},
  publisher={Now Publishers, Inc.}
}

@article{courty2017learning,
  title={Learning wasserstein embeddings},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Ducoffe, M{\'e}lanie},
  journal={arXiv preprint arXiv:1710.07457},
  year={2017}
}

@article{bonet2021subspace,
  title={Subspace Detours Meet Gromov--Wasserstein},
  author={Bonet, Cl{\'e}ment and Vayer, Titouan and Courty, Nicolas and Septier, Fran{\c{c}}ois and Drumetz, Lucas},
  journal={Algorithms},
  volume={14},
  number={12},
  pages={366},
  year={2021},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{nguyen2022amortized,
  title = {Amortized Projection Optimization for Sliced Wasserstein Generative Models},
  author = {Nguyen, Khai and Ho, Nhat},
  journal={arXiv preprint arXiv:2203.13417},
  year = {2022},
}

@article{dadashi2020primal,
  title={Primal wasserstein imitation learning},
  author={Dadashi, Robert and Hussenot, L{\'e}onard and Geist, Matthieu and Pietquin, Olivier},
  journal={arXiv preprint arXiv:2006.04678},
  year={2020}
}

@incollection{galichon2016optimal,
  title={Optimal transport methods in economics},
  author={Galichon, Alfred},
  booktitle={Optimal Transport Methods in Economics},
  year={2016},
  publisher={Princeton University Press}
}

@article{huang2020convex,
  title={Convex potential flows: Universal probability distributions with optimal transport and convex optimization},
  author={Huang, Chin-Wei and Chen, Ricky TQ and Tsirigotis, Christos and Courville, Aaron},
  journal={arXiv preprint arXiv:2012.05942},
  year={2020}
}

@inproceedings{cohen2021riemannian,
  title={Riemannian Convex Potential Maps},
  author={Cohen, Samuel and Amos, Brandon and Lipman, Yaron},
  booktitle={International Conference on Machine Learning},
  pages={2028--2038},
  year={2021},
  organization={PMLR}
}

@article{courty2017joint,
  title={Joint distribution optimal transportation for domain adaptation},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury and Rakotomamonjy, Alain},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{redko2019optimal,
  title={Optimal transport for multi-source domain adaptation under target shift},
  author={Redko, Ievgen and Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={849--858},
  year={2019},
  organization={PMLR}
}

@inproceedings{kolkin2019style,
  title={Style transfer by relaxed optimal transport and self-similarity},
  author={Kolkin, Nicholas and Salavon, Jason and Shakhnarovich, Gregory},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10051--10060},
  year={2019}
}

@article{kolouri2017optimal,
  title={Optimal mass transport: Signal processing and machine-learning applications},
  author={Kolouri, Soheil and Park, Se Rim and Thorpe, Matthew and Slepcev, Dejan and Rohde, Gustavo K},
  journal={IEEE signal processing magazine},
  volume={34},
  number={4},
  pages={43--59},
  year={2017},
  publisher={IEEE}
}

@inproceedings{kolouri2018sliced,
  title={Sliced Wasserstein auto-encoders},
  author={Kolouri, Soheil and Pope, Phillip E and Martin, Charles E and Rohde, Gustavo K},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{kolouri2016sliced,
  title={Sliced Wasserstein kernels for probability distributions},
  author={Kolouri, Soheil and Zou, Yang and Rohde, Gustavo K},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5258--5267},
  year={2016}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@article{cuturi2022optimal,
  title={Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein},
  author={Cuturi, Marco and Meng-Papaxanthos, Laetitia and Tian, Yingtao and Bunne, Charlotte and Davis, Geoff and Teboul, Olivier},
  journal={arXiv preprint arXiv:2201.12324},
  year={2022}
}


@article{flamary2021pot,
  title={Pot: Python optimal transport},
  author={Flamary, R{\'e}mi and Courty, Nicolas and Gramfort, Alexandre and Alaya, Mokhtar Z and Boisbunon, Aur{\'e}lie and Chambon, Stanislas and Chapel, Laetitia and Corenflos, Adrien and Fatras, Kilian and Fournier, Nemo and others},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={78},
  pages={1--8},
  year={2021}
}

@inproceedings{makkuva2020optimal,
  title={Optimal transport mapping via input convex neural networks},
  author={Makkuva, Ashok and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={6672--6681},
  year={2020},
  organization={PMLR}
}

@article{fenchel1949conjugate,
  title={On conjugate convex functions},
  author={Fenchel, Werner},
  journal={Canadian Journal of Mathematics},
  volume={1},
  number={1},
  pages={73--77},
  year={1949},
  publisher={Cambridge University Press}
}

@incollection{rockafellar2015convex,
  title={Convex analysis},
  author={Rockafellar, Ralph Tyrell},
  booktitle={Convex analysis},
  year={2015},
  publisher={Princeton university press}
}

@inproceedings{amos2017input,
  title={Input convex neural networks},
  author={Amos, Brandon and Xu, Lei and Kolter, J Zico},
  booktitle={International Conference on Machine Learning},
  pages={146--155},
  year={2017},
  organization={PMLR}
}

@article{brenier1991polar,
  title={Polar factorization and monotone rearrangement of vector-valued functions},
  author={Brenier, Yann},
  journal={Communications on pure and applied mathematics},
  volume={44},
  number={4},
  pages={375--417},
  year={1991},
  publisher={Wiley Online Library}
}

@article{taghvaei2019wasserstein,
  title={2-wasserstein approximation via restricted convex potentials with application to improved training for gans},
  author={Taghvaei, Amirhossein and Jalali, Amin},
  journal={arXiv preprint arXiv:1902.07197},
  year={2019}
}

@article{chartrand2009gradient,
  title={A gradient descent solution to the Monge-Kantorovich problem},
  author={Chartrand, Rick and Wohlberg, Brendt and Vixie, Kevin and Bollt, Erik},
  journal={Applied Mathematical Sciences},
  volume={3},
  number={22},
  pages={1071--1080},
  year={2009}
}

@inproceedings{zhu2017unpaired,
  title={Unpaired image-to-image translation using cycle-consistent adversarial networks},
  author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2223--2232},
  year={2017}
}

@article{chen2021learning,
  title={Learning to optimize: A primer and a benchmark},
  author={Chen, Tianlong and Chen, Xiaohan and Chen, Wuyang and Heaton, Howard and Liu, Jialin and Wang, Zhangyang and Yin, Wotao},
  journal={arXiv preprint arXiv:2103.12828},
  year={2021}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@incollection{ambrosio2003lecture,
  title={Lecture notes on optimal transport problems},
  author={Ambrosio, Luigi},
  booktitle={Mathematical aspects of evolving interfaces},
  pages={1--52},
  year={2003},
  publisher={Springer}
}

@article{santambrogio2015optimal,
  title={Optimal transport for applied mathematicians},
  author={Santambrogio, Filippo},
  journal={Birk{\"a}user, NY},
  volume={55},
  number={58-63},
  pages={94},
  year={2015},
  publisher={Springer}
}

@article{ha2016hypernetworks,
  title={Hypernetworks},
  author={Ha, David and Dai, Andrew and Le, Quoc V},
  journal={arXiv preprint arXiv:1609.09106},
  year={2016}
}

@article{stanley2009hypercube,
  title={A hypercube-based encoding for evolving large-scale neural networks},
  author={Stanley, Kenneth O and D'Ambrosio, David B and Gauci, Jason},
  journal={Artificial life},
  volume={15},
  number={2},
  pages={185--212},
  year={2009},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}

@inproceedings{he2016identity,
  title={Identity mappings in deep residual networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European conference on computer vision},
  pages={630--645},
  year={2016},
  organization={Springer}
}

@article{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
  journal = {GitHub},
}

@article{cominetti1994asymptotic,
  title={Asymptotic analysis of the exponential penalty trajectory in linear programming},
  author={Cominetti, Roberto and Mart{\'\i}n, J San},
  journal={Mathematical Programming},
  volume={67},
  number={1},
  pages={169--187},
  year={1994},
  publisher={Springer}
}

@article{altschuler2017near,
  title={Near-linear time approximation algorithms for optimal transport via Sinkhorn iteration},
  author={Altschuler, Jason and Niles-Weed, Jonathan and Rigollet, Philippe},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{alaya2019screening,
  title={Screening sinkhorn algorithm for regularized optimal transport},
  author={Alaya, Mokhtar Z and Berar, Maxime and Gasso, Gilles and Rakotomamonjy, Alain},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{thibault2017overrelaxed,
  title={Overrelaxed Sinkhorn-Knopp algorithm for regularized optimal transport},
  author={Thibault, Alexis and Chizat, Lenaic and Dossal, Charles and Papadakis, Nicolas},
  journal={arXiv preprint arXiv:1711.01851},
  year={2017}
}

@article{lin2019acceleration,
  title={On the acceleration of the Sinkhorn and Greenkhorn algorithms for optimal transport},
  author={Lin, Tianyi and Ho, Nhat and Jordan, Michael I},
  journal={arXiv preprint arXiv:1906.01437},
  year={2019}
}

@article{perrot2016mapping,
  title={Mapping estimation for discrete optimal transport},
  author={Perrot, Micha{\"e}l and Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{doxsey2015taking,
  title={Taking advantage of the improved availability of census data: a first look at the gridded population of the world, version 4},
  author={Doxsey-Whitfield, Erin and MacManus, Kytt and Adamo, Susana B and Pistolesi, Linda and Squires, John and Borkovska, Olena and Baptista, Sandra R},
  journal={Papers in Applied Geography},
  volume={1},
  number={3},
  pages={226--234},
  year={2015},
  publisher={Taylor \& Francis}
}

@article{sommerfeld2019optimal,
  title={Optimal Transport: Fast Probabilistic Approximation with Exact Solvers.},
  author={Sommerfeld, Max and Schrieber, J{\"o}rn and Zemel, Yoav and Munk, Axel},
  journal={J. Mach. Learn. Res.},
  volume={20},
  pages={105--1},
  year={2019}
}


@inproceedings{tancik2021learned,
  title={Learned initializations for optimizing coordinate-based neural representations},
  author={Tancik, Matthew and Mildenhall, Ben and Wang, Terrance and Schmidt, Divi and Srinivasan, Pratul P and Barron, Jonathan T and Ng, Ren},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2846--2855},
  year={2021}
}

@article{rusu2018meta,
  title={Meta-learning with latent embedding optimization},
  author={Rusu, Andrei A and Rao, Dushyant and Sygnowski, Jakub and Vinyals, Oriol and Pascanu, Razvan and Osindero, Simon and Hadsell, Raia},
  journal={arXiv preprint arXiv:1807.05960},
  year={2018}
}

@inproceedings{zintgraf2019fast,
  title={Fast context adaptation via meta-learning},
  author={Zintgraf, Luisa and Shiarli, Kyriacos and Kurin, Vitaly and Hofmann, Katja and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={7693--7702},
  year={2019},
  organization={PMLR}
}

@inproceedings{kluyver2016jupyter,
  author    = {Kluyver, Thomas and Ragan-Kelley, Benjamin and P{\'{e}}rez, Fernando and Granger, Brian E and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica B and Grout, Jason and Corlay, Sylvain and others},
  booktitle = {ELPUB},
  year = {2016},
  pages     = {87--90},
  title     = {Jupyter Notebooks-a publishing format for reproducible computational workflows.},
}


@article{hunter2007matplotlib,
  author       = {Hunter, John D},
  publisher    = {IEEE Computer Society},
  year = {2007},
  journal = {Computing in science \& engineering},
  number       = {3},
  pages        = {90},
  title        = {Matplotlib: A 2D graphics environment},
  volume       = {9},
}


@book{mckinney2012python,
  author    = {McKinney, Wes},
  publisher = {" O'Reilly Media, Inc."},
  year = {2012},
  title     = {Python for data analysis: Data wrangling with Pandas, NumPy, and IPython},
}


@article{jones2014scipy,
  author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
  year = {2014},
  title  = {{SciPy}: Open source scientific tools for {Python}},
}


@article{oliphant2007python,
  author       = {Oliphant, Travis E},
  publisher    = {IEEE},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  number       = {3},
  pages        = {10--20},
  title        = {Python for scientific computing},
  volume       = {9},
}

@book{oliphant2006guide,
  author    = {Oliphant, Travis E},
  publisher = {Trelgol Publishing USA},
  year = {2006},
  title     = {A guide to NumPy},
  volume    = {1},
}



@book{van1995python,
  author    = {Van Rossum, Guido and Drake Jr, Fred L},
  publisher = {Centrum voor Wiskunde en Informatica Amsterdam},
  year = {1995},
  title     = {Python reference manual},
}


@misc{seaborn,
  author       = {Michael Waskom and
                  Olga Botvinnik and
                  Drew O'Kane and
                  Paul Hobson and
                  Joel Ostblom and
                  Saulius Lukauskas and
                  David C Gemperline and
                  Tom Augspurger and
                  Yaroslav Halchenko and
                  John B. Cole and
                  Jordi Warmenhoven and
                  Julian de Ruiter and
                  Cameron Pye and
                  Stephan Hoyer and
                  Jake Vanderplas and
                  Santi Villalba and
                  Gero Kunter and
                  Eric Quintero and
                  Pete Bachant and
                  Marcel Martin and
                  Kyle Meyer and
                  Alistair Miles and
                  Yoav Ram and
                  Thomas Brunner and
                  Tal Yarkoni and
                  Mike Lee Williams and
                  Constantine Evans and
                  Clark Fitzgerald and
                  Brian and
                  Adel Qalieh},
  title        = {mwaskom/seaborn: v0.9.0 (July 2018)},
  month        = jul,
  year         = 2018,
  doi          = {10.5281/zenodo.1313201},
  url          = {https://doi.org/10.5281/zenodo.1313201}
}


@article{van2011numpy,
  author       = {Van Der Walt, Stefan and Colbert, S Chris and Varoquaux, Gael},
  publisher    = {IEEE Computer Society},
  year = {2011},
  journal = {Computing in Science \& Engineering},
  number       = {2},
  pages        = {22},
  title        = {The NumPy array: a structure for efficient numerical computation},
  volume       = {13},
}

@Misc{Yadan2019Hydra,
  author =       {Omry Yadan},
  title =        {Hydra - A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year =         {2019},
  url =          {https://github.com/facebookresearch/hydra}
}

@article{engquist2013application,
  title={Application of the Wasserstein metric to seismic signals},
  author={Engquist, Bjorn and Froese, Brittany D},
  journal={arXiv preprint arXiv:1311.4581},
  year={2013}
}

@article{bunne2022supervised,
  title={Supervised Training of Conditional Monge Maps},
  author={Bunne, Charlotte and Krause, Andreas and Cuturi, Marco},
  journal={arXiv preprint arXiv:2206.14262},
  year={2022}
}

@article{bunne2021learning,
  title={Learning Single-Cell Perturbation Responses using Neural Optimal Transport},
  author={Bunne, Charlotte and Stark, Stefan G and Gut, Gabriele and del Castillo, Jacobo Sarabia and Lehmann, Kjong-Van and Pelkmans, Lucas and Krause, Andreas and Ratsch, Gunnar},
  journal={bioRxiv},
  year={2021},
  publisher={Cold Spring Harbor Laboratory}
}

@inproceedings{bunne2022proximal,
  title={Proximal Optimal Transport Modeling of Population Dynamics},
  author={Bunne, Charlotte and Papaxanthos, Laetitia and Krause, Andreas and Cuturi, Marco},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={6511--6528},
  year={2022},
  organization={PMLR}
}

@article{thornton2022rethinking,
  title={Rethinking Initialization of the Sinkhorn Algorithm},
  author={Thornton, James and Cuturi, Marco},
  journal={arXiv preprint arXiv:2206.07630},
  year={2022}
}